# -*- coding: utf-8 -*-

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMPORTS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Standard library imports
from collections import *
from glob import glob
import warnings
import datetime

# Third party imports
import numpy as np
import pandas as pd
import pysam as ps

# Local lib import
from pycoQC.common import *

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~GLOBAL SETTINGS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Silence futurewarnings
warnings.filterwarnings("ignore", category=FutureWarning)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~MAIN CLASS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
class pycoQC_parse ():

    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~INIT METHOD~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    def __init__ (self,
        summary_file:str,
        barcode_file:str="",
        bam_file:str="",
        runid_list:list=[],
        filter_calibration:bool=False,
        filter_duplicated:bool=False,
        min_barcode_percent:float=0.1,
        verbose:bool=False,
        quiet:bool=False):
        """
        Parse Albacore sequencing_summary.txt file and clean-up the data
        * summary_file
            Path to the sequencing_summary generated by Albacore 1.0.0 + (read_fast5_basecaller.py) / Guppy 2.1.3+ (guppy_basecaller).
            One can also pass multiple space separated file paths or a UNIX style regex matching multiple files
        * barcode_file
            Path to the barcode_file generated by Guppy 2.1.3+ (guppy_barcoder) or Deepbinner 0.2.0+. This is not a required file.
            One can also pass multiple space separated file paths or a UNIX style regex matching multiple files
        * bam_file
            Path to a Bam file corresponding to reads in the summary_file. Preferably aligned with Minimap2
            One can also pass multiple space separated file paths or a UNIX style regex matching multiple files
        * runid_list
            Select only specific runids to be analysed. Can also be used to force pycoQC to order the runids for
            temporal plots, if the sequencing_summary file contain several sucessive runs. By default pycoQC analyses
            all the runids in the file and uses the runid order as defined in the file.
        * filter_calibration
            If True read flagged as calibration strand by the software are removed
        * filter_duplicated
            If True duplicated read_ids are removed but the first occurence is kept (Guppy sometimes outputs the same read multiple times)
        * min_barcode_percent
            Minimal percent of total reads to retain barcode label. If below the barcode value is set as `unclassified`.
        """

        # Set logging level
        self.logger = get_logger(name=__name__, verbose=verbose, quiet=quiet)

        # Init object counter
        self.counter = OrderedDict()

        # Import summary, barcode and bam files and merge data
        self.logger.warning ("Parsing input files")
        summary_reads_df = self._parse_summary(summary_file)
        barcode_reads_df = self._parse_barcode(barcode_file)
        bam_reads_df, ref_df = self._parse_bam(bam_file)
        reads_df = self._merge_reads_df(summary_reads_df, barcode_reads_df, bam_reads_df)

        # Cleanup data
        self.logger.warning("Cleaning data")
        self.reads_df = self._clean_reads_df(reads_df, runid_list, filter_calibration, min_barcode_percent)
        self.ref_df = ref_df

    def __str__(self):
        return dict_to_str(self.counter)

    def __repr__(self):
        return "[{}]\n".format(self.__class__.__name__)

    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~PRIVATE METHODS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

    def _parse_summary (self, summary_file):
        """"""
        self.logger.info ("\tImporting sequencing information from sequencing summary files")
        summary_files_list = self._expand_file_names(summary_file)
        self.logger.debug ("\t\tSequencing summary files found: {}".format(" ".join(summary_files_list)))
        self.counter["Summary files found"] = len(summary_files_list)

        df = self._merge_files_to_df (summary_files_list)
        # Define specific parameters depending on the run_type
        self.logger.debug ("\tVerifying fields and discarding unused columns")

        if "sequence_length_template" in df:
            self.logger.debug ("\t\t1D Run type")
            self.run_type = "1D"
            required_colnames = ["read_id", "run_id", "channel", "start_time", "sequence_length_template", "mean_qscore_template"]
            optional_colnames = ["calibration_strand_genome_template", "barcode_arrangement"]
            rename_colmanes = {
                "sequence_length_template":"num_bases", "mean_qscore_template":"mean_qscore",
                "calibration_strand_genome_template":"calibration","barcode_arrangement":"barcode"}

        elif "sequence_length_2d" in df:
            self.logger.debug ("\t\t1D2 Run type")
            self.run_type = "1D2"
            required_colnames = ["read_id", "run_id", "channel", "start_time", "sequence_length_2d", "mean_qscore_2d"]
            optional_colnames = ["calibration_strand_genome_template", "barcode_arrangement"]
            rename_colmanes = {
                "sequence_length_2d":"num_bases", "mean_qscore_2d":"mean_qscore",
                "calibration_strand_genome_template":"calibration", "barcode_arrangement":"barcode"}
        else:
            raise pycoQCError ("Invalid sequencing summary file")

        # Verify the required and optional columns, Drop unused fields and standardise field names.
        col = self._check_df_columns (df=df, required_colnames=required_colnames, optional_colnames=optional_colnames)
        self.logger.debug ("\t\tColumns found: {}".format(col))
        df = df[col]
        df = df.rename(columns=rename_colmanes)

        # Collect stats
        n = len(df)
        self.logger.debug ("\t\t{:,} reads found in initial file".format(n))
        self.counter["Initial reads"] = n

        return df

    def _parse_barcode (self, barcode_file):
        """"""
        if not barcode_file:
            return pd.DataFrame()

        self.logger.info ("\tImporting barcode information from barcode summary files")
        barcode_files_list = self._expand_file_names(barcode_file)
        self.logger.debug ("\t\tBarcode summary files found: {}".format(" ".join(barcode_files_list)))
        self.counter["Barcode files found"] = len(barcode_files_list)

        df = self._merge_files_to_df (barcode_files_list)

        # check presence of barcode details
        if "read_id" in df and "barcode_arrangement" in df:
            self.logger.debug ("\t\tFound valid Guppy barcode file")
            df = df [["read_id", "barcode_arrangement"]]
            df = df.rename(columns={"barcode_arrangement":"barcode"})

        elif "read_ID" in df and "barcode_call" in df:
            self.logger.debug ("\t\tFound valid Deepbinner barcode file")
            df = df [["read_ID", "barcode_call"]]
            df = df.rename(columns={"read_ID":"read_id", "barcode_call":"barcode"})
            df['barcode'].replace("none", "unclassified", inplace=True)
        else:
            raise pycoQCError ("File {} does not contain required barcode information".format(fp))

        n = len(df[df['barcode']!="unclassified"])
        self.logger.debug ("\t\t{:,} reads with barcodes assigned".format(n))
        self.counter["Reads with barcodes"] = n

        return df

    def _parse_bam (self, bam_file):
        """"""
        if not bam_file:
            return (pd.DataFrame(), pd.DataFrame())

        self.logger.info ("\tImporting alignment information from BAM files")

        bam_file_list = self._expand_file_names(bam_file)
        self.logger.debug ("\t\tBam files found: {}".format(" ".join(bam_file_list)))
        self.counter["Bam files found"] = len(bam_file_list)

        # Init collections
        unmapped=0
        ref_dict = OrderedDict()
        read_dict = OrderedDict ()

        for bam_fn in bam_file_list:
            with ps.AlignmentFile(bam_fn, "rb") as bam:

                # Check bam file
                if not bam.has_index():
                    raise pycoQCError("No index found for bam file: {}. Please index with samtools index".format(bam_fn))
                if not bam.header['HD']['SO'] == 'coordinate':
                    raise pycoQCError("Bam file not sorted: {}. Please sort with samtools sort".format(bam_fn))

                # Save reference level information
                for ref_id, ref_len in zip(bam.references, bam.lengths):
                    if not ref_id in ref_dict:
                        ref_dict[ref_id] = Counter()
                        ref_dict[ref_id]["length"]=ref_len

                # Parse reads
                for read in bam:
                    if read.is_unmapped:
                        unmapped+=1
                    elif read.is_secondary:
                        ref_dict[read.reference_name]["secondary alignments"]+=1
                    elif read.is_supplementary:
                        ref_dict[read.reference_name]["suplementary alignments"]+=1
                    elif read.query_name in read_dict:
                        ref_dict[read.reference_name]["duplicated read_id"]+=1
                    else:
                        ref_dict[read.reference_name]["primary alignments"]+=1
                        ref_dict[read.reference_name]["base aligned"]+=read.query_alignment_length
                        read_dict[read.query_name] = self._get_read_stats(read)

        # Convert ref and read dict to dataframes
        if ref_dict:
            ref_df = pd.DataFrame.from_dict(ref_dict, orient="index")
            ref_df.index.name="ref_id"
            ref_df.reset_index(inplace=True)
            ref_df.fillna(0, inplace=True)
            ref_df = ref_df.astype(int)
        else:
            ref_df = pd.DataFrame()

        if read_dict:
            read_df = pd.DataFrame.from_dict(read_dict, orient="index")
            read_df.index.name="read_id"
            read_df.reset_index(inplace=True)
        else:
            read_df = pd.DataFrame()

        return (read_df, ref_df)

    def _merge_reads_df(self, summary_reads_df, barcode_reads_df, bam_reads_df):
        """"""

        df = summary_reads_df

        # Merge df and fill in missing barcode values
        if not barcode_reads_df.empty:
            df = pd.merge(df, barcode_reads_df, on="read_id", how="left")
            df['barcode'].fillna('unclassified', inplace=True)

        # Merge df and fill in missing barcode values
        if not bam_reads_df.empty:
            df = pd.merge(df, bam_reads_df, on="read_id", how="left")

        return df

    def _clean_reads_df (self, df, runid_list, filter_calibration, min_barcode_percent):
        """"""
        # Drop lines containing NA values
        self.logger.info ("\tDiscarding lines containing NA values")
        l = len(df)
        df = df.dropna(subset=["read_id", "run_id"])
        n=l-len(df)
        self.logger.info ("\t\t{:,} reads discarded".format(n))
        self.counter["Reads with NA values discarded"] = n
        if len(df) <= 1:
            raise pycoQCError("No valid read left after NA values filtering")

        # Filter out zero length reads
        self.logger.info ("\tFiltering out zero length reads")
        l = len(df)
        df = df[(df["num_bases"] > 0)]
        n=l-len(df)
        self.logger.info ("\t\t{:,} reads discarded".format(n))
        self.counter["Zero length reads discarded"] = n
        if len(df) <= 1:
            raise pycoQCError("No valid read left after zero_len filtering")

        # Filter out calibration strands read if the "calibration_strand_genome_template" field is available
        if filter_duplicated:
            self.logger.info ("\tFiltering out duplicated reads")
            l = len(df)
            df = df[~df.duplicated(subset="read_id", keep='first')]
            n=l-len(df)
            self.logger.info ("\t\t{:,} reads discarded".format(n))
            self.counter["Duplicated reads discarded"] = n
            if len(df) <= 1:
                raise pycoQCError("No valid read left after calibration strand filtering")

        # Filter out calibration strands read if the "calibration_strand_genome_template" field is available
        if filter_calibration and "calibration" in df:
            self.logger.info ("\tFiltering out calibration strand reads")
            l = len(df)
            df = df[(df["calibration"].isin(["filtered_out", "no_match", "*"]))]
            n=l-len(df)
            self.logger.info ("\t\t{:,} reads discarded".format(n))
            self.counter["Calibration reads discarded"] = n
            if len(df) <= 1:
                raise pycoQCError("No valid read left after calibration strand filtering")

        # Filter and reorder based on runid_list list if passed by user
        if runid_list:
            self.logger.info ("\tSelecting run_ids passed by user")
            l = len(df)
            df = df[(df["run_id"].isin(runid_list))]
            n=l-len(df)
            self.logger.debug ("\t\t{:,} reads discarded".format(n))
            self.counter["Excluded runid reads discarded"] = n
            if len(df) <= 1:
                raise pycoQCError("No valid read left after run ID filtering")
            runid_list = runid_list

        # Else sort the runids by output per time assuming that the throughput decreases over time
        else:
            self.logger.info ("\tSorting run IDs by decreasing throughput")
            d = {}
            for run_id, sdf in df.groupby("run_id"):
                d[run_id] = len(sdf)/np.ptp(sdf["start_time"])
            runid_list = [i for i, j in sorted (d.items(), key=lambda t: t[1], reverse=True)]
            self.logger.info ("\t\tRun-id order {}".format(runid_list))

        # Modify start time per run ids to order them following the runid_list
        self.logger.info ("\tReordering runids")
        increment_time = 0
        runid_start = OrderedDict()
        for runid in runid_list:
            self.logger.info ("\t\tProcessing reads with Run_ID {} / time offset: {}".format(runid, increment_time))
            max_val = df['start_time'][df["run_id"] == runid].max()
            df.loc[df["run_id"] == runid, 'start_time'] += increment_time
            runid_start[runid] = increment_time
            increment_time += max_val+1
        df = df.sort_values ("start_time")

        #  Unset low frequency barcodes
        if "barcode" in df:
            self.logger.info ("\tCleaning up low frequency barcodes")
            l = (df["barcode"]=="unclassified").sum()
            barcode_counts = df["barcode"][df["barcode"]!="unclassified"].value_counts()
            cutoff = int(barcode_counts.sum()*min_barcode_percent/100)
            low_barcode = barcode_counts[barcode_counts<cutoff].index
            df.loc[df["barcode"].isin(low_barcode), "barcode"] = "unclassified"
            n= int((df["barcode"]=="unclassified").sum()-l)
            self.logger.info ("\t\t{:,} reads with low frequency barcode unset".format(n))
            self.counter["Reads with low frequency barcode unset"] = n

        # Reindex final df
        self.logger.info ("\tReindexing dataframe by read_ids")
        df = df.reset_index (drop=True)
        df = df.set_index ("read_id")
        self.logger.info ("\t\t{:,} Final valid reads".format(len(df)))

        # Save final df
        self.counter["Valid reads"] = len(df)
        if len(df) < 500:
            self.logger.warning ("WARNING: Low number of reads found. This is likely to lead to errors when trying to generate plots")

        return df

    #~~~~~~~~~~~~~~~~~~~~~~~~~~PRIVATE STATIC METHODS~~~~~~~~~~~~~~~~~~~~~~~~~~#

    @staticmethod
    def _check_df_columns(df, required_colnames, optional_colnames):
        """"""
        col_found = []
        # Verify the presence of the columns required for pycoQC
        for col in required_colnames:
            if col in df:
                col_found.append(col)
            else:
                raise pycoQCError("Column {} not found in the provided sequence_summary file".format(col))
        for col in optional_colnames:
            if col in df:
                col_found.append(col)
        return col_found

    @staticmethod
    def _expand_file_names(fn):
        """"""
        if not fn:
            return []

        # Try to expand file name to list
        if isinstance(fn, list):
            if len(fn) ==1:
                fn_list=glob(fn[0])
            else:
                fn_list=fn
        elif isinstance(fn, str):
            fn_list=glob(fn)
        else:
            raise pycoQCError ("{} has to be either a file or a regular expression or a list of files".format(fn))

        # Verify that a least 1 file was found and that files are readable
        if len(fn_list) == 0:
            raise pycoQCError ("{} does not correspond to any valid file".format(fn))
        for fn in fn_list:
            if not is_readable_file (fn):
                raise pycoQCError("Cannot read file {}".format(fn))

        return fn_list

    @staticmethod
    def _merge_files_to_df(fn_list):
        """"""
        if len(fn_list) == 1:
            df =  pd.read_csv(fn_list[0], sep ="\t")

        else:
            df_list = []
            for fn in fn_list:
                df_list.append (pd.read_csv(fn, sep ="\t"))
            df = pd.concat(df_list, ignore_index=True, sort=False, join="inner")

        if len(df) == 0:
            raise pycoQCError ("No valid read found in input file")

        return df

    @staticmethod
    def _get_read_stats(read):
        """"""
        d = OrderedDict()

        # Extract general stats
        d["ref_id"] = read.reference_name
        d["ref_start"] = read.reference_start
        d["ref_end"] = read.reference_end
        d["align_len"] = read.query_alignment_length
        d["mapq"] = read.mapping_quality

        # Get edit distance from NM tag if set up
        if read.has_tag("NM"):
            d["edit_dist"] = read.get_tag("NM")

        # Extract indel and soft_clip from cigar
        c_stat = read.get_cigar_stats()[0]
        d["insertion"] = c_stat[1]
        d["deletion"] = c_stat[2]
        d["soft_clip"] = c_stat[4]

        # Extract mismatch from MD tag
        if read.has_tag("MD"):
            md_err = 0
            for i in read.get_tag("MD"):
                if i in ["A","T","C","G"]:
                    md_err += 1
            d["mismatch"] = md_err-d["deletion"]

        return d
